#!/usr/bin/env python3
"""Aggregation report generator for judged evaluation results.

Reads judged JSON files and produces a markdown comparison report.

Usage:
    # Report from all judged results
    python scripts/report.py

    # Report from specific files
    python scripts/report.py evaluations/judged/Q4.1_*.json
"""

from __future__ import annotations

import argparse
import json
import sys
from datetime import datetime, timezone
from pathlib import Path

ROOT = Path(__file__).resolve().parent.parent
JUDGED_DIR = ROOT / "evaluations" / "judged"
REPORTS_DIR = ROOT / "evaluations" / "reports"

DIMENSIONS = [
    "specificity",
    "internal_consistency",
    "tension_surfacing",
    "constraint_awareness",
    "actionability",
    "reasoning_depth",
    "completeness",
]


def load_judged(files: list[Path]) -> list[dict]:
    """Load judged result JSON files."""
    results = []
    for f in files:
        results.append(json.loads(f.read_text()))
    return results


def render_report(results: list[dict]) -> str:
    """Render a markdown report from judged results."""
    sections = [
        "# Coordination Lab Evaluation Report\n",
        _render_score_table(results),
        _render_ranking_summary(results),
        _render_per_question(results),
        _render_methodology(),
    ]
    return "\n\n".join(sections)


def _render_score_table(results: list[dict]) -> str:
    """Mean score per protocol per dimension."""
    lines = ["## Score Table (Mean Across Questions)\n"]

    agg: dict[str, dict[str, list[float]]] = {}
    for r in results:
        for protocol, dim_scores in r.get("scores", {}).items():
            agg.setdefault(protocol, {})
            for dim, score in dim_scores.items():
                agg[protocol].setdefault(dim, []).append(score)

    if not agg:
        return lines[0] + "_No scores available._"

    dim_short = [d[:14] for d in DIMENSIONS]
    header = "| Protocol | " + " | ".join(dim_short) + " | Mean |"
    sep = "|----------|" + "|".join(["------:" for _ in DIMENSIONS]) + "|------:|"
    lines.extend([header, sep])

    for protocol in sorted(agg):
        vals = []
        for dim in DIMENSIONS:
            scores = agg[protocol].get(dim, [])
            mean = sum(scores) / len(scores) if scores else 0
            vals.append(f"{mean:.1f}")
        all_scores = [s for dl in agg[protocol].values() for s in dl]
        overall = sum(all_scores) / len(all_scores) if all_scores else 0
        row = f"| {protocol} | " + " | ".join(vals) + f" | **{overall:.2f}** |"
        lines.append(row)

    return "\n".join(lines)


def _render_ranking_summary(results: list[dict]) -> str:
    """First-place wins across questions."""
    lines = ["## Forced Ranking Summary\n"]

    first_place: dict[str, int] = {}
    for r in results:
        ranking = r.get("ranking", [])
        if ranking:
            winner = ranking[0]
            first_place[winner] = first_place.get(winner, 0) + 1

    if not first_place:
        return lines[0] + "_No rankings available._"

    header = "| Protocol | First-Place Wins |"
    sep = "|----------|----------------:|"
    lines.extend([header, sep])

    for protocol in sorted(first_place, key=first_place.get, reverse=True):
        lines.append(f"| {protocol} | {first_place[protocol]} |")

    return "\n".join(lines)


def _render_per_question(results: list[dict]) -> str:
    """Per-question breakdown."""
    lines = ["## Per-Question Results\n"]

    for r in results:
        qid = r.get("question_id", "unknown")
        lines.append(f"### {qid}\n")

        scores = r.get("scores", {})
        if not scores:
            lines.append("_No scores._\n")
            continue

        header = "| Protocol | Mean Score |"
        sep = "|----------|----------:|"
        lines.extend([header, sep])

        for protocol, dim_scores in sorted(scores.items()):
            mean = sum(dim_scores.values()) / len(dim_scores) if dim_scores else 0
            lines.append(f"| {protocol} | {mean:.2f} |")

        ranking = r.get("ranking", [])
        if ranking:
            lines.append(f"\n**Ranking:** {' > '.join(ranking)}")
        reasoning = r.get("judge_reasoning", "")
        if reasoning:
            lines.append(f"\n**Judge notes:** {reasoning}")
        lines.append("")

    return "\n".join(lines)


def _render_methodology() -> str:
    return """## Methodology

- **7 evaluation dimensions** scored 1-5 by a blind Opus judge
- Outputs anonymized and randomized before judging
- Protocol-identifying tokens stripped for blind evaluation
- Dimensions: specificity, internal consistency, tension surfacing, constraint awareness, actionability, reasoning depth, completeness

*Generated by Coordination Lab Evaluation Framework*"""


def main() -> None:
    parser = argparse.ArgumentParser(description="Generate evaluation report")
    parser.add_argument("files", nargs="*", help="Judged JSON files (default: all in evaluations/judged/)")
    args = parser.parse_args()

    if args.files:
        files = [Path(f) for f in args.files]
    else:
        if not JUDGED_DIR.exists():
            print("No judged results found. Run judge.py first.")
            sys.exit(1)
        files = sorted(JUDGED_DIR.glob("*.json"))

    if not files:
        print("No judged files found.")
        sys.exit(1)

    results = load_judged(files)
    report = render_report(results)

    REPORTS_DIR.mkdir(parents=True, exist_ok=True)
    timestamp = datetime.now(timezone.utc).strftime("%Y%m%d_%H%M%S")
    outpath = REPORTS_DIR / f"report_{timestamp}.md"
    outpath.write_text(report)

    print(report)
    print(f"\nSaved to {outpath}")


if __name__ == "__main__":
    main()
